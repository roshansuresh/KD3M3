{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygmo import hypervolume\n",
    "import csv\n",
    "import statistics\n",
    "import numpy as np\n",
    "import operator as op\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Modified from Pau's code\n",
    "def compute_pareto_front(population):\n",
    "    pop_size = len(population)\n",
    "    obj_num = 2\n",
    "\n",
    "    domination_counter = [0] * pop_size\n",
    "\n",
    "    for i in range(pop_size):\n",
    "        for j in range(i+1, pop_size):\n",
    "            # check each objective for dominance\n",
    "            dominate = [0] * obj_num\n",
    "            for k in range(obj_num):\n",
    "                if population[i][k] > population[j][k]:\n",
    "                    dominate[k] = 1\n",
    "                elif population[i][k] < population[j][k]:\n",
    "                    dominate[k] = -1\n",
    "            if -1 not in dominate and 1 in dominate:\n",
    "                domination_counter[i] += 1\n",
    "            elif -1 in dominate and 1 not in dominate:\n",
    "                domination_counter[j] += 1\n",
    "\n",
    "    pareto_solutions = []\n",
    "    for i in range(len(domination_counter)):\n",
    "        if domination_counter[i] == 0:\n",
    "            pareto_solutions.append(population[i])\n",
    "    return pareto_solutions\n",
    "\n",
    "def compute_hv(population):\n",
    "    array_archs = np.zeros((len(population), 2))\n",
    "    for i in range(len(population)):\n",
    "        array_archs[i] = population[i]\n",
    "    hv_object = hypervolume(array_archs)\n",
    "    hv = hv_object.compute([1.1,1.1])/1.1**2\n",
    "    return hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Useful functions and parameter defintions\n",
    "nfe_interval = 50\n",
    "    \n",
    "def nchoosek(n,k):\n",
    "    k = min(k, n-k)\n",
    "    num = reduce(op.mul, range(n,n-k,-1), 1)\n",
    "    den = reduce(op.mul, range(1,k+1), 1)\n",
    "    return num/den\n",
    "    \n",
    "def get_true_objectives(true_obj1_array, true_obj2_array, index):\n",
    "    return true_obj1_array[index], true_obj2_array[index]\n",
    "\n",
    "def get_feasibility_score(feas_array, index):\n",
    "    return feas_array[index]\n",
    "\n",
    "def get_connectivity_score(conn_array, index):\n",
    "    return conn_array[index]\n",
    "\n",
    "def get_stiffness_ratio(stiffrat_array, index):\n",
    "    return stiffrat_array[index]\n",
    "\n",
    "def get_partcoll_score(partcoll_array, index):\n",
    "    return partcoll_array[index]\n",
    "\n",
    "def get_nodprop_score(nodprop_array, index):\n",
    "    return nodprop_array[index]\n",
    "\n",
    "def get_orientation_score(orient_array, index):\n",
    "    return orient_array[index]\n",
    "\n",
    "def get_design(design_array, index):\n",
    "    return design_array[index]\n",
    "\n",
    "def find_last_index(val,search_list):\n",
    "    return len(search_list) - search_list[::-1].index(val) - 1\n",
    "\n",
    "def find_closest_index(val,search_list):\n",
    "    val_diff = np.array(search_list) - val\n",
    "    closest_index = np.argmin(np.abs(val_diff))\n",
    "    return closest_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Determine csv filepath from given case type\n",
    "def get_csv_filepath(partcoll_constrained, nodprop_constrained, orient_constrained, fib_stiff, optim_problem2, read_constrad, run_number):\n",
    "    # partcoll_constrained = [int_pen, AOS, bias_init, ACH] boolean array\n",
    "    # nodprop_constrained = [int_pen, AOS, bias_init, ACH] boolean array\n",
    "    # orient_constrained = [int_pen, AOS, bias_init, ACH] boolean array\n",
    "    # optim_problem2 - boolean to signify reading of optimization problem 2 data\n",
    "    # read_constrad - boolean to signify reading of constant radii or variable radii data\n",
    "    \n",
    "    filepath = 'C:\\\\SEAK Lab\\\\SEAK Lab Github\\\\KD3M3\\\\Truss_AOS\\\\result\\\\'\n",
    "    methods = ['Int Pen','AOS','Bias Init','ACH']\n",
    "    if (partcoll_constrained[1] or nodprop_constrained[1] or orient_constrained[1]):\n",
    "        filename = 'AOSMOEA_emoea_'\n",
    "    else:\n",
    "        filename = 'EpsilonMOEA_emoea_'\n",
    "        \n",
    "    if optim_problem2:\n",
    "        filepath_prob = 'OA run data - optimization problem 2\\\\'\n",
    "        if read_constrad:\n",
    "            filename_prob = '_prob2'\n",
    "        else:\n",
    "            filename_prob = ''\n",
    "    else:\n",
    "        filepath_prob = 'OA run data - optimization problem 1\\\\'\n",
    "        filename_prob = ''\n",
    "        \n",
    "    if read_constrad:\n",
    "        filepath_constrad = 'Constant Radii\\\\'\n",
    "    else:\n",
    "        filepath_constrad = 'Variable Radii\\\\'\n",
    "        \n",
    "    filepath2 = ''\n",
    "    filename2 = ''\n",
    "    constr_count = 0\n",
    "    for i in range(4):\n",
    "        if(partcoll_constrained[i] and nodprop_constrained[i] and (not orient_constrained[i])):\n",
    "            constrained = methods[i] + ' - PartColl and NodalProp\\\\'\n",
    "            filename_snippet = 'pncon' + str(i) + '_'\n",
    "        elif(partcoll_constrained[i] and (not nodprop_constrained[i]) and (not orient_constrained[i])):\n",
    "            constrained = methods[i] + ' - PartColl\\\\'\n",
    "            filename_snippet = 'pcon' + str(i) + '_'\n",
    "        elif((not partcoll_constrained[i]) and nodprop_constrained[i] and (not orient_constrained[i])):\n",
    "            constrained = methods[i] + ' - NodalProp\\\\'    \n",
    "            filename_snippet = 'ncon' + str(i) + '_'\n",
    "        elif((not partcoll_constrained[i]) and (not nodprop_constrained[i]) and orient_constrained[i]):\n",
    "            constrained = methods[i] + ' - Orientation\\\\'    \n",
    "            filename_snippet = 'ocon' + str(i) + '_'\n",
    "        elif(partcoll_constrained[i] and (not nodprop_constrained[i]) and orient_constrained[i]):\n",
    "            constrained = methods[i] + ' - PartColl and Orientation\\\\'    \n",
    "            filename_snippet = 'pocon' + str(i) + '_'\n",
    "        elif((not partcoll_constrained[i]) and nodprop_constrained[i] and orient_constrained[i]):\n",
    "            constrained = methods[i] + ' - NodalProp and Orientation\\\\'    \n",
    "            filename_snippet = 'nocon' + str(i) + '_'\n",
    "        elif(partcoll_constrained[i] and nodprop_constrained[i] and orient_constrained[i]):\n",
    "            constrained = methods[i] + ' - PartColl NodalProp and Orientation\\\\'    \n",
    "            filename_snippet = 'pnocon' + str(i) + '_'\n",
    "        else:\n",
    "            constrained = ''\n",
    "            filename_snippet = ''\n",
    "            constr_count += 1\n",
    "        filepath2 += constrained\n",
    "        filename2 += filename_snippet\n",
    "        \n",
    "    filepath_moea = ''\n",
    "    if (constr_count == 4):\n",
    "        filepath_moea = 'Epsilon MOEA\\\\'\n",
    "        \n",
    "    if fib_stiff:\n",
    "        filepath3 = 'Fibre Stiffness\\\\'\n",
    "        if read_constrad:\n",
    "            filename_model = '_fibre_fullpop.csv'\n",
    "        else:\n",
    "            filename_model = '_fibre_varrad_fullpop.csv'\n",
    "    else:\n",
    "        filepath3 = 'Truss Stiffness\\\\'\n",
    "        if read_constrad:\n",
    "            filename_model = '_truss_fullpop.csv'\n",
    "        else: \n",
    "            filename_model = '_truss_varrad_fullpop.csv'\n",
    "        \n",
    "    return filepath + filepath_prob + filepath_constrad + filepath2 + filepath_moea + filepath3 + filename + str(run_number) + filename2 + filename_prob + filename_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Extract Pareto Front and normalization constants data from csv file\n",
    "def extract_data_from_csv(csv_filepath, fib_stif, constrad_bool, sidenum):\n",
    "    n_total_members = int(nchoosek(sidenum**2,2))\n",
    "    with open(csv_filepath,newline='') as csvfile:\n",
    "        data = [row for row in csv.reader(csvfile)]\n",
    "        if (constrad_bool):\n",
    "            designs_dat = [\"\" for x in range(len(data)-1)]\n",
    "        else:\n",
    "            designs_dat = np.zeros((len(data)-1,n_total_members))\n",
    "                \n",
    "        num_func_evals_dat = np.zeros(len(data)-1)\n",
    "        pen_obj1_dat = np.zeros(len(data)-1)\n",
    "        pen_obj2_dat = np.zeros(len(data)-1)\n",
    "        true_obj1_dat = np.zeros(len(data)-1)\n",
    "        true_obj2_dat = np.zeros(len(data)-1)\n",
    "        feas_scores_dat = np.zeros(len(data)-1)\n",
    "        conn_scores_dat = np.zeros(len(data)-1)\n",
    "        stiffrat_vals_dat = np.zeros(len(data)-1)\n",
    "        partcoll_scores_dat = np.zeros(len(data)-1)\n",
    "        nodprop_scores_dat = np.zeros(len(data)-1)\n",
    "        orient_scores_dat = np.zeros(len(data)-1)\n",
    "        valid_count = 0\n",
    "        for x in range(len(data)-1):\n",
    "            data_float = list(map(float,data[x+1][1:]))\n",
    "            if (any(np.isnan(np.array(data_float))) or any(np.isinf(np.array(data_float)))):\n",
    "                continue\n",
    "            if (constrad_bool):\n",
    "                designs_dat[valid_count] = data[x+1][0]\n",
    "            else:\n",
    "                designs_dat[valid_count] = data[x+1][:n_total_members]\n",
    "                        \n",
    "            num_func_evals_dat[valid_count] = int(data[x+1][-11])\n",
    "            pen_obj1_dat[valid_count] = float(data[x+1][-10])\n",
    "            pen_obj2_dat[valid_count] = float(data[x+1][-9])\n",
    "            true_obj1_dat[valid_count] = float(data[x+1][-8])\n",
    "            true_obj2_dat[valid_count] = float(data[x+1][-7])\n",
    "            feas_scores_dat[valid_count] = float(data[x+1][-6])\n",
    "            conn_scores_dat[valid_count] = float(data[x+1][-5])\n",
    "            stiffrat_vals_dat[valid_count] = float(data[x+1][-4])\n",
    "            partcoll_scores_dat[valid_count] = float(data[x+1][-3])\n",
    "            nodprop_scores_dat[valid_count] = float(data[x+1][-2])\n",
    "            orient_scores_dat[valid_count] = float(data[x+1][-1])\n",
    "            valid_count += 1\n",
    "            \n",
    "    designs = designs_dat[:valid_count]\n",
    "    num_func_evals = num_func_evals_dat[:valid_count]\n",
    "    pen_obj1 = pen_obj1_dat[:valid_count]\n",
    "    pen_obj2 = pen_obj2_dat[:valid_count]\n",
    "    true_obj1 = true_obj1_dat[:valid_count]\n",
    "    true_obj2 = true_obj2_dat[:valid_count]\n",
    "    feas_scores =  feas_scores_dat[:valid_count]\n",
    "    conn_scores = conn_scores_dat[:valid_count]\n",
    "    stiffrat_vals = stiffrat_vals_dat[:valid_count]\n",
    "    partcoll_scores = partcoll_scores_dat[:valid_count]\n",
    "    nodprop_scores = nodprop_scores_dat[:valid_count]\n",
    "    orient_scores = orient_scores_dat[:valid_count]\n",
    "            \n",
    "    ## Sort num_fun_evals (and obj1 & obj2, feas and stab scores) in ascending order\n",
    "    n_func_evals = num_func_evals\n",
    "    sort_indices = np.argsort(n_func_evals)\n",
    "    pen_obj1_sorted = list(pen_obj1[sort_indices])\n",
    "    pen_obj2_sorted = list(pen_obj2[sort_indices])\n",
    "    true_obj1_sorted = list(true_obj1[sort_indices])\n",
    "    true_obj2_sorted = list(true_obj2[sort_indices])\n",
    "    feas_scores_sorted = list(feas_scores[sort_indices])\n",
    "    conn_scores_sorted = list(conn_scores[sort_indices])\n",
    "    stiffrat_vals_sorted = list(stiffrat_vals[sort_indices])\n",
    "    partcoll_scores_sorted = list(partcoll_scores[sort_indices])\n",
    "    nodprop_scores_sorted = list(nodprop_scores[sort_indices])\n",
    "    orient_scores_sorted = list(orient_scores[sort_indices])\n",
    "    \n",
    "\n",
    "    designs_sorted = []\n",
    "    for i in range(len(sort_indices)):\n",
    "        designs_sorted.append(designs[sort_indices[i]])\n",
    "    \n",
    "    nfe_list_sorted = list(n_func_evals[sort_indices])\n",
    "    \n",
    "    ## Determine normalizing objective scores for true and penalized objectives \n",
    "    #max_func_evals = nfe_list_sorted[-1]\n",
    "    max_func_evals = 10000 # some runs for some cases run upto 10001 evaluations, which causes hv array length issues\n",
    "\n",
    "    obj_normalize_max_fullrun = [np.max(pen_obj1_sorted), np.max(pen_obj2_sorted)]\n",
    "    obj_normalize_min_fullrun = [np.min(pen_obj1_sorted), np.min(pen_obj2_sorted)]\n",
    "\n",
    "    obj_true_normalize_max_fullrun = [np.max(true_obj1_sorted), np.max(true_obj2_sorted)]\n",
    "    obj_true_normalize_min_fullrun = [np.min(true_obj1_sorted), np.min(true_obj2_sorted)]\n",
    "\n",
    "    obj1_normalize_max_afterjump = 0\n",
    "    obj1_normalize_min_afterjump = 0\n",
    "    obj2_normalize_max_afterjump = 0\n",
    "    obj2_normalize_min_afterjump = 0\n",
    "\n",
    "    pareto_front_dict = {}\n",
    "    pareto_front_feas_dict = {}\n",
    "    pareto_front_conn_dict = {}\n",
    "    pareto_front_stiffrat_dict = {}\n",
    "    pareto_front_partcoll_dict = {}\n",
    "    pareto_front_nodprop_dict = {}\n",
    "    pareto_front_orient_dict = {}\n",
    "    pareto_front_designs_dict = {}\n",
    "    pareto_front_true_dict = {}\n",
    "    count = 0\n",
    "    pop_size = int(find_last_index(0, nfe_list_sorted))\n",
    "    nfe_jump_recorded = False\n",
    "    jump_nfe = 0\n",
    "\n",
    "    for i in range(0, int(max_func_evals), nfe_interval):\n",
    "        #print('iter = ' + str(i))\n",
    "    \n",
    "        if (i < 100):\n",
    "            nfe_index_current = pop_size\n",
    "        else:\n",
    "            nfe_index_current = find_closest_index(i, nfe_list_sorted)\n",
    "        \n",
    "        nfe_array_current = nfe_list_sorted[:nfe_index_current]\n",
    "        current_population = []\n",
    "        for j in range(len(nfe_array_current)):\n",
    "            current_population.append([pen_obj1_sorted[j], pen_obj2_sorted[j]])\n",
    "\n",
    "        current_pareto_front_all = compute_pareto_front(current_population)\n",
    "        #current_pareto_front = list(set(current_pareto_front_all))\n",
    "        current_pareto_front = np.unique(current_pareto_front_all, axis=0)\n",
    "    \n",
    "        current_pareto_feas_scores = []\n",
    "        current_pareto_conn_scores = []\n",
    "        current_pareto_stiffrat_vals = []\n",
    "        current_pareto_partcoll_scores = []\n",
    "        current_pareto_nodprop_scores = []\n",
    "        current_pareto_orient_scores = []\n",
    "        current_pareto_designs = []\n",
    "        current_pareto_true_obj = []\n",
    "        for pareto_design in current_pareto_front:\n",
    "            design_index = pen_obj1_sorted.index(pareto_design[0])\n",
    "            design_feas_score = get_feasibility_score(feas_scores_sorted, design_index)\n",
    "            design_conn_score = get_connectivity_score(conn_scores_sorted, design_index)\n",
    "            design_stiffrat_val = get_stiffness_ratio(stiffrat_vals_sorted, design_index)\n",
    "            design_partcoll_score = get_partcoll_score(partcoll_scores_sorted, design_index)\n",
    "            design_nodprop_score = get_nodprop_score(nodprop_scores_sorted, design_index)\n",
    "            design_orient_score = get_orientation_score(orient_scores_sorted, design_index)\n",
    "            current_pareto_feas_scores.append(design_feas_score)\n",
    "            current_pareto_conn_scores.append(design_conn_score)\n",
    "            current_pareto_stiffrat_vals.append(design_stiffrat_val)\n",
    "            current_pareto_partcoll_scores.append(design_partcoll_score)\n",
    "            current_pareto_nodprop_scores.append(design_nodprop_score)\n",
    "            current_pareto_orient_scores.append(design_orient_score)\n",
    "            current_pareto_designs.append(get_design(designs_sorted, design_index))\n",
    "            \n",
    "            #true_obj1, true_obj2 = compute_true_objectives(pareto_design[0], pareto_design[1], design_feas_score, design_stab_score, fib_stif, intpen_feas_bool, intpen_stab_bool)\n",
    "            true_obj1, true_obj2 = get_true_objectives(true_obj1_sorted, true_obj2_sorted, design_index)\n",
    "            \n",
    "            #print('pareto_design')\n",
    "            #print(pareto_design)\n",
    "            #print('true_objs')\n",
    "            #print([true_obj1, true_obj2])\n",
    "            #print('design_feas_score')\n",
    "            #print(design_feas_score)\n",
    "            #print('design_stab_score')\n",
    "            #print(design_stab_score)\n",
    "            #print('design_orient_score')\n",
    "            #print(design_orient_score)\n",
    "            #print('intpen_feas_bool')\n",
    "            #print(intpen_feas_bool)\n",
    "            #print('intpen_stab_bool')\n",
    "            #print(intpen_stab_bool)\n",
    "            #print('intpen_orient_bool')\n",
    "            #print(intpen_orient_bool)\n",
    "            #print('current_pareto_design')\n",
    "            #print(get_design(designs_sorted, design_index))\n",
    "            #print('\\n')\n",
    "            #set_trace()\n",
    "            \n",
    "            current_pareto_true_obj.append([true_obj1, true_obj2])\n",
    "        \n",
    "        pareto_front_dict[i] = current_pareto_front\n",
    "        pareto_front_feas_dict[i] = current_pareto_feas_scores\n",
    "        pareto_front_conn_dict[i] = current_pareto_conn_scores\n",
    "        pareto_front_stiffrat_dict[i] = current_pareto_stiffrat_vals\n",
    "        pareto_front_partcoll_dict[i] = current_pareto_partcoll_scores\n",
    "        pareto_front_nodprop_dict[i] = current_pareto_nodprop_scores\n",
    "        pareto_front_orient_dict[i] = current_pareto_orient_scores\n",
    "        pareto_front_designs_dict[i] = current_pareto_designs\n",
    "        pareto_front_true_dict[i] = current_pareto_true_obj\n",
    "    \n",
    "        #nonzero_feas_scores = True in (feas_score > 0.1 for feas_score in current_pareto_feas_scores)\n",
    "        #if (nonzero_feas_scores):\n",
    "            #if (not nfe_jump_recorded):\n",
    "                #jump_nfe = i\n",
    "                #nfe_jump_recorded = True\n",
    "        \n",
    "            #pareto_obj1s = [pareto_design[0] for pareto_design in current_pareto_front]\n",
    "            #pareto_obj2s = [pareto_design[1] for pareto_design in current_pareto_front]\n",
    "        \n",
    "            #if (np.max(pareto_obj1s) > obj1_normalize_max_afterjump):\n",
    "                #obj1_normalize_max_afterjump = np.max(pareto_obj1s)\n",
    "        \n",
    "            #if (np.max(pareto_obj2s) > obj2_normalize_max_afterjump):\n",
    "                #obj2_normalize_max_afterjump = np.max(pareto_obj2s)\n",
    "        \n",
    "            #if (np.min(pareto_obj1s) < obj1_normalize_min_afterjump):\n",
    "                #obj1_normalize_min_afterjump = np.min(pareto_obj1s)\n",
    "        \n",
    "            #if (np.min(pareto_obj2s) < obj2_normalize_min_afterjump):\n",
    "                #obj2_normalize_min_afterjump = np.min(pareto_obj2s)\n",
    "\n",
    "    obj_normalize_max_afterjump = [obj1_normalize_max_afterjump, obj2_normalize_max_afterjump]\n",
    "    obj_normalize_min_afterjump = [obj1_normalize_min_afterjump, obj2_normalize_min_afterjump]\n",
    "    \n",
    "    obj_normalize_fullrun = [obj_normalize_min_fullrun, obj_normalize_max_fullrun]\n",
    "    obj_normalize_afterjump = [obj_normalize_min_afterjump, obj_normalize_max_afterjump]\n",
    "    obj_normalize_true_fullrun = [obj_true_normalize_min_fullrun, obj_true_normalize_max_fullrun]\n",
    "    \n",
    "    return pareto_front_dict, pareto_front_true_dict, obj_normalize_fullrun, obj_normalize_afterjump, obj_normalize_true_fullrun, jump_nfe, max_func_evals, nfe_jump_recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute overall normalization objectives for single case study/all compared case studies\n",
    "def compute_overall_norm_objs(objs_normalization_full, objs_normalization_afterjump, objs_normalization_true):\n",
    "    # Each input is a dictionary with key as the case study/run string and value as the corresponding 2D array\n",
    "    \n",
    "    obj1_max_full_allcases = np.zeros(len(objs_normalization_full))\n",
    "    obj1_min_full_allcases = np.zeros(len(objs_normalization_full))\n",
    "    obj2_max_full_allcases = np.zeros(len(objs_normalization_full))\n",
    "    obj2_min_full_allcases = np.zeros(len(objs_normalization_full))\n",
    "    obj1_max_aj_allcases = np.zeros(len(objs_normalization_afterjump))\n",
    "    obj1_min_aj_allcases = np.zeros(len(objs_normalization_afterjump))\n",
    "    obj2_max_aj_allcases = np.zeros(len(objs_normalization_afterjump))\n",
    "    obj2_min_aj_allcases = np.zeros(len(objs_normalization_afterjump))\n",
    "    obj1_max_true_allcases = np.zeros(len(objs_normalization_true))\n",
    "    obj1_min_true_allcases = np.zeros(len(objs_normalization_true))\n",
    "    obj2_max_true_allcases = np.zeros(len(objs_normalization_true))\n",
    "    obj2_min_true_allcases = np.zeros(len(objs_normalization_true))\n",
    "    \n",
    "    i = 0\n",
    "    for key in objs_normalization_full:\n",
    "        current_objs_norm_full = objs_normalization_full[key]\n",
    "        \n",
    "        obj1_max_full_allcases[i] = current_objs_norm_full[1][0]\n",
    "        obj2_max_full_allcases[i] = current_objs_norm_full[1][1]\n",
    "        obj1_min_full_allcases[i] = current_objs_norm_full[0][0]\n",
    "        obj2_min_full_allcases[i] = current_objs_norm_full[0][1]\n",
    "        i += 1\n",
    "        \n",
    "    i = 0\n",
    "    for key2 in objs_normalization_afterjump:\n",
    "        current_objs_norm_afterjump = objs_normalization_afterjump[key2]\n",
    "        \n",
    "        obj1_max_aj_allcases[i] = current_objs_norm_afterjump[1][0]\n",
    "        obj2_max_aj_allcases[i] = current_objs_norm_afterjump[1][1]\n",
    "        obj1_min_aj_allcases[i] = current_objs_norm_afterjump[0][0]\n",
    "        obj2_min_aj_allcases[i] = current_objs_norm_afterjump[0][1]\n",
    "        i += 1\n",
    "        \n",
    "    i = 0\n",
    "    for key3 in objs_normalization_true:\n",
    "        current_objs_norm_true = objs_normalization_true[key3]\n",
    "        \n",
    "        obj1_max_true_allcases[i] = current_objs_norm_true[1][0]\n",
    "        obj2_max_true_allcases[i] = current_objs_norm_true[1][1]\n",
    "        obj1_min_true_allcases[i] = current_objs_norm_true[0][0]\n",
    "        obj2_min_true_allcases[i] = current_objs_norm_true[0][1]\n",
    "        i += 1\n",
    "        \n",
    "    obj1_min_full_overall = np.min(obj1_min_full_allcases)\n",
    "    obj2_min_full_overall = np.min(obj2_min_full_allcases)\n",
    "    obj1_max_full_overall = np.max(obj1_max_full_allcases)\n",
    "    obj2_max_full_overall = np.max(obj2_max_full_allcases)\n",
    "    \n",
    "    obj1_min_aj_overall = np.min(obj1_min_aj_allcases)\n",
    "    obj2_min_aj_overall = np.min(obj2_min_aj_allcases)\n",
    "    obj1_max_aj_overall = np.max(obj1_max_aj_allcases)\n",
    "    obj2_max_aj_overall = np.max(obj2_max_aj_allcases)\n",
    "    \n",
    "    obj1_min_true_overall = np.min(obj1_min_true_allcases)\n",
    "    obj2_min_true_overall = np.min(obj2_min_true_allcases)\n",
    "    obj1_max_true_overall = np.max(obj1_max_true_allcases)\n",
    "    obj2_max_true_overall = np.max(obj2_max_true_allcases)\n",
    "            \n",
    "    obj_norm_full_overall = [[obj1_min_full_overall, obj2_min_full_overall], [obj1_max_full_overall, obj2_max_full_overall]]\n",
    "    obj_norm_aj_overall = [[obj1_min_aj_overall, obj2_min_aj_overall], [obj1_max_aj_overall, obj2_max_aj_overall]]\n",
    "    obj_norm_true_overall = [[obj1_min_true_overall, obj2_min_true_overall], [obj1_max_true_overall, obj2_max_true_overall]]    \n",
    "    \n",
    "    return obj_norm_full_overall, obj_norm_aj_overall, obj_norm_true_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute hypervolume arrays from copmuted pareto fronts and normalization constants\n",
    "def compute_hv_arrays_from_csv_data(pf_dict, pf_true_dict, obj_norm_full, obj_norm_afterjump, obj_norm_true_full, nfe_jump, max_fun_evals, nfe_jump_achieved):\n",
    "    obj_norm_min_full = obj_norm_full[0]\n",
    "    obj_norm_max_full = obj_norm_full[1]\n",
    "    #obj_norm_min_afterjump = obj_norm_afterjump[0]\n",
    "    #obj_norm_max_afterjump = obj_norm_afterjump[1]\n",
    "    obj_norm_true_min_full = obj_norm_true_full[0]\n",
    "    obj_norm_true_max_full = obj_norm_true_full[1]\n",
    "\n",
    "    ## Normalize the pareto front objectives and compute the hypervolume\n",
    "    hypervol_full_dict = []\n",
    "    hypervol_true_full_dict = []\n",
    "    hypervol_afterjump_dict = []\n",
    "\n",
    "    for i in range(0, int(max_fun_evals), nfe_interval):\n",
    "        #print('iter = ' + str(i))\n",
    "    \n",
    "        current_pareto_front = pf_dict[i]\n",
    "        current_true_pareto_front = pf_true_dict[i]\n",
    "        current_pf_normalized = []\n",
    "        #current_pf_normalized_afterjump = []\n",
    "        current_pf_true_normalized = []\n",
    "        for pareto_design in current_pareto_front:\n",
    "            obj1_normalized = (pareto_design[0] - obj_norm_min_full[0])/(obj_norm_max_full[0] - obj_norm_min_full[0])\n",
    "            obj2_normalized = (pareto_design[1] - obj_norm_min_full[1])/(obj_norm_max_full[1] - obj_norm_min_full[1])\n",
    "            current_pf_normalized.append([obj1_normalized, obj2_normalized])\n",
    "            #if ((i >= nfe_jump) and nfe_jump_achieved):\n",
    "                #obj1_normalized_afterjump = (pareto_design[0] - obj_norm_min_afterjump[0])/(obj_norm_max_afterjump[0] - obj_norm_min_afterjump[0])\n",
    "                #obj2_normalized_afterjump = (pareto_design[1] - obj_norm_min_afterjump[1])/(obj_norm_max_afterjump[1] - obj_norm_min_afterjump[1])\n",
    "                #current_pf_normalized_afterjump.append([obj1_normalized_afterjump, obj2_normalized_afterjump])\n",
    "            \n",
    "        for pareto_design_true in current_true_pareto_front:\n",
    "            obj1_true_normalized = (obj_norm_true_max_full[0] - pareto_design_true[0])/(obj_norm_true_max_full[0] - obj_norm_true_min_full[0])\n",
    "            obj2_true_normalized = (pareto_design_true[1] - obj_norm_true_min_full[1])/(obj_norm_true_max_full[1] - obj_norm_true_min_full[1])\n",
    "            current_pf_true_normalized.append([obj1_true_normalized, obj2_true_normalized])\n",
    "            \n",
    "        current_hv = compute_hv(current_pf_normalized)\n",
    "        hypervol_full_dict.append([i, current_hv])\n",
    "        #if ((i >= nfe_jump) and nfe_jump_achieved):\n",
    "            #current_hv_afterjump = compute_hv(current_pf_normalized_afterjump)\n",
    "            #hypervol_afterjump_dict.append([i, current_hv_afterjump])\n",
    "        \n",
    "        current_hv_true = compute_hv(current_pf_true_normalized)\n",
    "        hypervol_true_full_dict.append([i, current_hv_true])\n",
    "        \n",
    "    return hypervol_full_dict, hypervol_true_full_dict, hypervol_afterjump_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hypervolume_stats(hypervols_dict):\n",
    "    hv_dict_keys = list(hypervols_dict.keys())\n",
    "    hv_dict_0 = hypervols_dict[hv_dict_keys[0]]\n",
    "    nfe_array_0 = [hv_array[0] for hv_array in hv_dict_0]\n",
    "    n_datapoints = len(nfe_array_0)\n",
    "    hypervol_median = np.zeros(n_datapoints)\n",
    "    hypervol_1q = np.zeros(n_datapoints)\n",
    "    hypervol_3q = np.zeros(n_datapoints)\n",
    "    for i in range(n_datapoints):\n",
    "        hypervol_vals = []\n",
    "        for key in hypervols_dict:\n",
    "            hv_dict_j = hypervols_dict[key]\n",
    "            hv_current_array = [hv_array[1] for hv_array in hv_dict_j]\n",
    "            hypervol_vals.append(hv_current_array[i])\n",
    "        hypervol_median[i] = statistics.median(hypervol_vals)\n",
    "        hypervol_1q[i] = np.percentile(hypervol_vals, 25)\n",
    "        hypervol_3q[i] = np.percentile(hypervol_vals, 75)\n",
    "        \n",
    "    return hypervol_median, hypervol_1q, hypervol_3q, nfe_array_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hypervolume_stats(hv_median_case, hv_1q_case, hv_3q_case, nfe_array, savefig_name):\n",
    "    fig1 = plt.figure(1)\n",
    "    plt.plot(nfe_array,hv_median_case, 'b-', label='Median')\n",
    "    plt.plot(nfe_array,hv_1q_case, 'r-', label='1st Quartile')\n",
    "    plt.plot(nfe_array,hv_3q_case, 'g-', label='3rd Quartile')\n",
    "    plt.xlabel('Number of Function Evaluations')\n",
    "    plt.ylabel('Hypervolume')\n",
    "    plt.title('Averaged Hypervolume vs NFE')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    #fig1.savefig('HV_plot_averaged_' + savefig_name + '.png')\n",
    "    \n",
    "def plot_hypervolume_stats_allcases(hv_median_dict, hv_1q_dict, hv_3q_dict, nfe_array, colour_array, alpha_array, casename_array, savefig_name):\n",
    "    fig1 = plt.figure(1)\n",
    "    number_cases = len(hv_median_dict)\n",
    "    #print('n_cases')\n",
    "    #print(number_cases)\n",
    "    for i in range(number_cases):\n",
    "        #print(print(marker_array[i]+'*'))\n",
    "        plt.plot(nfe_array, hv_median_dict['case'+str(i)], '-', color=colour_array[i], label=casename_array[i])\n",
    "        plt.fill_between(nfe_array, hv_1q_dict['case'+str(i)], hv_3q_dict['case'+str(i)], color=colour_array[i], alpha=alpha_array[i])\n",
    "        \n",
    "        #plt.plot(nfe_array, hv_1q_dict['case'+str(i)], '--', color=colour_array[i], label=casename_array[i]+' 1st Quartile')\n",
    "        #plt.plot(nfe_array, hv_3q_dict['case'+str(i)], '--', color=colour_array[i], label=casename_array[i]+' 3rd Quartile')\n",
    "    plt.xlabel('Number of Function Evaluations')\n",
    "    plt.ylabel('Hypervolume')\n",
    "    plt.title('Averaged Hypervolume vs NFE')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.04,0.5), borderaxespad=0)\n",
    "    plt.show()\n",
    "    #fig1.savefig('HV_plot_averaged_' + savefig_name + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define functions to compute and plot hypervolume for single case and all cases\n",
    "def hypervolume_computation_single_case(fib_stiffness, case_booleans, optim_prob2_bool, constrad_read_bool, sidenum, run_nums, case_name):\n",
    "    ## Computing the pareto fronts and normalization objectives for each run\n",
    "    obj_norm_allruns = {}\n",
    "    obj_norm_afterjump_allruns = {}\n",
    "    obj_norm_true_allruns = {}\n",
    "    pf_allruns = {}\n",
    "    pf_true_allruns = {}\n",
    "    nfe_jump_allruns = np.zeros(run_nums)\n",
    "    max_f_evals_allruns = np.zeros(run_nums)\n",
    "    jump_rec_allruns = [False for x in range(run_nums)]\n",
    "    for i in range(run_nums):\n",
    "        print('Computing Pareto Fronts for run ' + str(i))\n",
    "        current_csvpath = get_csv_filepath(case_booleans[:4], case_booleans[4:8], case_booleans[8:12], case_booleans[-2], optim_prob2_bool, constrad_read_bool, case_booleans[-1], i)\n",
    "        pf_dict_i, pf_true_dict_i, obj_norm_full_i, obj_norm_afterjump_i, obj_norm_true_i, nfe_jump_i, max_fun_evals_i, nfe_jump_recorded_i = extract_data_from_csv(current_csvpath, fib_stiffness, constrad_read_bool, sidenum)\n",
    "        pf_allruns['run'+str(i)] = pf_dict_i\n",
    "        pf_true_allruns['run'+str(i)] = pf_true_dict_i\n",
    "        obj_norm_allruns['run'+str(i)] = obj_norm_full_i\n",
    "        obj_norm_afterjump_allruns['run'+str(i)] = obj_norm_afterjump_i\n",
    "        obj_norm_true_allruns['run'+str(i)] = obj_norm_true_i\n",
    "        nfe_jump_allruns[i] = nfe_jump_i\n",
    "        max_f_evals_allruns[i] = max_fun_evals_i\n",
    "        jump_rec_allruns[i] = nfe_jump_recorded_i\n",
    "\n",
    "    ## Use computed normalization objectives and find the overall normalization objectives across all runs\n",
    "    print('Computing overall normalization constants')\n",
    "    norm_objs_full_overall, norm_objs_aj_overall, norm_objs_true_overall = compute_overall_norm_objs(obj_norm_allruns, obj_norm_afterjump_allruns, obj_norm_true_allruns)\n",
    "    print('norm_objs_full_overall')\n",
    "    print(norm_objs_full_overall)\n",
    "    print('\\n')\n",
    "    print('norm_objs_true_overall')\n",
    "    print(norm_objs_true_overall)\n",
    "    print('\\n')\n",
    "    \n",
    "    ## Compute Hypervolume values for each run\n",
    "    hv_dict_allruns = {}\n",
    "    #hv_dict_aj_allruns = {}\n",
    "    hv_dict_true_allruns = {}\n",
    "    for j in range(run_nums):\n",
    "        print('Computing hypervolume values for run ' + str(j))\n",
    "        hv_dict_j, hv_dict_true_j, hv_dict_aj_j = compute_hv_arrays_from_csv_data(pf_allruns['run'+str(j)], pf_true_allruns['run'+str(j)], norm_objs_full_overall, norm_objs_aj_overall, norm_objs_true_overall, nfe_jump_allruns[j], max_f_evals_allruns[j], jump_rec_allruns[j])\n",
    "        hv_dict_allruns['run'+str(j)] = hv_dict_j\n",
    "        #hv_dict_aj_allruns['run'+str(j)] = hv_dict_aj_j\n",
    "        hv_dict_true_allruns['run'+str(j)] = hv_dict_true_j\n",
    "        \n",
    "    print('hv_dict_allruns')\n",
    "    print(hv_dict_allruns)\n",
    "    print('\\n')\n",
    "    print('hv_dict_true_allruns')\n",
    "    print(hv_dict_true_allruns)\n",
    "    print('\\n')\n",
    "    \n",
    "    ## Plotting\n",
    "    print('Plotting')\n",
    "    hv_median_all, hv_1q_all, hv_3q_all, nfe_array = compute_hypervolume_stats(hv_dict_allruns)\n",
    "    plot_hypervolume_stats(hv_median_all, hv_1q_all, hv_3q_all, nfe_array, case_name+'_full')\n",
    "\n",
    "    ## Plot HVs for hv_afterjump\n",
    "\n",
    "    hv_true_median_all, hv_true_1q_all, hv_true_3q_all, nfe_array_true = compute_hypervolume_stats(hv_dict_true_allruns)\n",
    "    plot_hypervolume_stats(hv_true_median_all, hv_true_1q_all, hv_true_3q_all, nfe_array_true, case_name+'_true')\n",
    "    \n",
    "def hypervolume_computation_all_cases(fib_stiffness, case_bools_dict, optim_prob2_bool, sidenum, run_nums, marker_colours, alpha_vals, case_names):\n",
    "    num_cases = len(case_bools_dict) # number of cases to compare \n",
    "\n",
    "    ## Computing the pareto fronts and normalization objectives for each run in each case\n",
    "    pf_allcases = {}\n",
    "    pf_true_allcases = {}\n",
    "    obj_norm_allcasesandruns = {}\n",
    "    obj_norm_afterjump_allcasesandruns = {}\n",
    "    obj_norm_true_allcasesandruns = {}\n",
    "    nfe_jump_allcases = {}\n",
    "    max_f_evals_allcases = {}\n",
    "    jump_recorded_allcases = {}\n",
    "    for i in range(num_cases):\n",
    "        print('Computing Pareto Fronts for runs in Case '+str(i))\n",
    "        current_case_bools = case_bools_dict['case'+str(i+1)]\n",
    "        pf_allruns_i = {}\n",
    "        pf_true_allruns_i = {}\n",
    "        nfe_jump_allruns = np.zeros(run_nums)\n",
    "        max_f_evals_allruns = np.zeros(run_nums)\n",
    "        jump_rec_allruns = [False for x in range(run_nums)]\n",
    "        for j in range(run_nums):\n",
    "            print('Run '+str(j))\n",
    "            current_csvpath = get_csv_filepath(current_case_bools[:4], current_case_bools[4:8], current_case_bools[8:12], current_case_bools[-2], optim_prob2_bool, current_case_bools[-1], j)\n",
    "            pf_dict_j, pf_true_dict_j, obj_norm_full_j, obj_norm_afterjump_j, obj_norm_true_j, nfe_jump_j, max_fun_evals_j, nfe_jump_recorded_j = extract_data_from_csv(current_csvpath, fib_stiffness, current_case_bools[-1], sidenum)\n",
    "            pf_allruns_i['run'+str(j)] = pf_dict_j\n",
    "            pf_true_allruns_i['run'+str(j)] = pf_true_dict_j\n",
    "            obj_norm_allcasesandruns['case'+str(i+1)+'run'+str(j)] = obj_norm_full_j\n",
    "            obj_norm_afterjump_allcasesandruns['case'+str(i+1)+'run'+str(j)] = obj_norm_afterjump_j\n",
    "            obj_norm_true_allcasesandruns['case'+str(i+1)+'run'+str(j)] = obj_norm_true_j\n",
    "            nfe_jump_allruns[j] = nfe_jump_j\n",
    "            max_f_evals_allruns[j] = max_fun_evals_j\n",
    "            jump_rec_allruns[j] = nfe_jump_recorded_j\n",
    "        pf_allcases['case'+str(i+1)] = pf_allruns_i\n",
    "        pf_true_allcases['case'+str(i+1)] = pf_true_allruns_i\n",
    "        nfe_jump_allcases['case'+str(i+1)] = nfe_jump_allruns\n",
    "        max_f_evals_allcases['case'+str(i+1)] = max_f_evals_allruns\n",
    "        jump_recorded_allcases['case'+str(i+1)] = jump_rec_allruns\n",
    "    \n",
    "    ## Use computed normalization objectives and find the overall normalization objectives across all runs and cases\n",
    "    print('Computing overall normalization constants')\n",
    "    norm_objs_full_overall, norm_objs_aj_overall, norm_objs_true_overall = compute_overall_norm_objs(obj_norm_allcasesandruns, obj_norm_afterjump_allcasesandruns, obj_norm_true_allcasesandruns)\n",
    "    print('norm_objs_full_overall')\n",
    "    print(norm_objs_full_overall)\n",
    "    print('\\n')\n",
    "    print('norm_objs_true_overall')\n",
    "    print(norm_objs_true_overall)\n",
    "    print('\\n') \n",
    "    \n",
    "    #set_trace()\n",
    "    \n",
    "    ## Compute Hypervolume values for each run in each case\n",
    "    hv_dict_median_allcases = {}\n",
    "    hv_dict_1q_allcases = {}\n",
    "    hv_dict_3q_allcases = {}\n",
    "    #hv_dict_aj_median_allcases = {}\n",
    "    #hv_dict_aj_1q_allcases = {}\n",
    "    #hv_dict_aj_3q_allcases = {}\n",
    "    hv_dict_true_median_allcases = {}\n",
    "    hv_dict_true_1q_allcases = {}\n",
    "    hv_dict_true_3q_allcases = {}\n",
    "    for i in range(num_cases):\n",
    "        print('Computing hypervolume values for runs in Case '+str(i))\n",
    "        pfs_case_i = pf_allcases['case'+str(i+1)]\n",
    "        pfs_true_case_i = pf_true_allcases['case'+str(i+1)]\n",
    "        nfes_jump_case_i = nfe_jump_allcases['case'+str(i+1)]\n",
    "        max_func_evals_i = max_f_evals_allcases['case'+str(i+1)]\n",
    "        jump_recorded_i = jump_recorded_allcases['case'+str(i+1)]\n",
    "        hv_dict_allruns = {}\n",
    "        #hv_dict_aj_allruns = {}\n",
    "        hv_dict_true_allruns = {}\n",
    "        for j in range(run_nums):\n",
    "            print('Run '+str(j))\n",
    "            hv_dict_j, hv_dict_true_j, hv_dict_aj_j = compute_hv_arrays_from_csv_data(pfs_case_i['run'+str(j)], pfs_true_case_i['run'+str(j)], norm_objs_full_overall, norm_objs_aj_overall, norm_objs_true_overall, nfes_jump_case_i[j], max_func_evals_i[j], jump_recorded_i[j])\n",
    "            hv_dict_allruns['run'+str(j)] = hv_dict_j\n",
    "            #hv_dict_aj_allruns['run'+str(j)] = hv_dict_aj_j\n",
    "            hv_dict_true_allruns['run'+str(j)] = hv_dict_true_j\n",
    "        \n",
    "        #if (i == 7):\n",
    "            #print('length of hv arrays')\n",
    "            #for j in range(run_nums):\n",
    "                #print(len(hv_dict_true_allruns['run'+str(j)]))\n",
    "            #set_trace()\n",
    "                \n",
    "        print('Computing hypervolume stats')\n",
    "        hv_med_i, hv_1q_i, hv_3q_i, nfe_array_i = compute_hypervolume_stats(hv_dict_allruns)\n",
    "        #hv_med_aj_i, hv_1q_aj_i, hv_3q_aj_i, nfe_array_aj_i = compute_hypervolume_stats(hv_dict_aj_allruns)\n",
    "        hv_med_true_i, hv_1q_true_i, hv_3q_true_i, nfe_array_true_i = compute_hypervolume_stats(hv_dict_true_allruns)\n",
    "        hv_dict_median_allcases['case'+str(i)] = hv_med_i\n",
    "        hv_dict_1q_allcases['case'+str(i)] = hv_1q_i\n",
    "        hv_dict_3q_allcases['case'+str(i)] = hv_3q_i\n",
    "        #hv_dict_aj_median_allcases['case'+str(i)] = hv_med_aj_i\n",
    "        #hv_dict_aj_1q_allcases['case'+str(i)] = hv_1q_aj_i\n",
    "        #hv_dict_aj_3q_allcases['case'+str(i)] = hv_3q_aj_i\n",
    "        hv_dict_true_median_allcases['case'+str(i)] = hv_med_true_i\n",
    "        hv_dict_true_1q_allcases['case'+str(i)] = hv_1q_true_i\n",
    "        hv_dict_true_3q_allcases['case'+str(i)] = hv_3q_true_i\n",
    "        \n",
    "    #print('hv_dict_allruns')\n",
    "    #print(hv_dict_allruns)\n",
    "    #print('\\n')\n",
    "    #print('hv_dict_true_allruns')\n",
    "    #print(hv_dict_true_allruns)\n",
    "    #print('\\n')\n",
    "        \n",
    "    ## Plotting\n",
    "    print('Plotting')\n",
    "    plot_hypervolume_stats_allcases(hv_dict_median_allcases, hv_dict_1q_allcases, hv_dict_3q_allcases, nfe_array_i, marker_colours, alpha_vals, case_names, 'allcases_full')\n",
    "\n",
    "    ## Plot HVs for hv_afterjump\n",
    "\n",
    "    plot_hypervolume_stats_allcases(hv_dict_true_median_allcases, hv_dict_true_1q_allcases, hv_dict_true_3q_allcases, nfe_array_true_i, marker_colours, alpha_vals, case_names, 'allcases_true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "fibre_stiffness = False\n",
    "optimization_prob2_bool = True # boolean to read optimization problem2 data\n",
    "constant_rad_data_read = False # Read constant radii problem data\n",
    "sidenum = 3 # 3x3 node grid\n",
    "cases_dict = {}\n",
    "num_runs = 30 # number of runs for each case\n",
    "\n",
    "# bools = [int_pen_partcoll, AOS_partcoll, bias_init_partcoll, ACH_partcoll, int_pen_nodalprop, AOS_nodalprop, bias_init_nodalprop, ACH_nodalprop, int_pen_orient, AOS_orient, bias_init_orient, ACH_orient, fibre_stiffness, with_norm]\n",
    "case1_bools = [False, False, False, False, False, False, False, False, False, False, False, False, fibre_stiffness, constant_rad_data_read] # Simple E-MOEA, Constant Radii problem\n",
    "case2_bools = [False, False, False, False, False, False, False, False, False, False, False, False, fibre_stiffness, not constant_rad_data_read] # Simple E-MOEA, Variable Radii problem\n",
    "cases_dict['case1'] = case1_bools\n",
    "cases_dict['case2'] = case2_bools\n",
    "\n",
    "line_colours = ['xkcd:black','xkcd:blue']\n",
    "casenames = ['Constant Radii','Variable Radii']\n",
    "alpha_values = [0.5,0.5] # change based on number of cases/visibility\n",
    "\n",
    "hypervolume_computation_all_cases(fibre_stiffness, cases_dict, optimization_prob2_bool, sidenum, num_runs, line_colours, alpha_values, casenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
